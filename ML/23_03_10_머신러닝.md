# 머신러닝

- 명시적으로 데이터를 프로그래밍 해 학습시키는 것

- 데이터를 입힘으로서 성능이 향상되는것

---

## 머신러닝의 분류

1. `SUPERVISED LEARNING `(지도학습)
   - 데이터에 정답이 있는 경우 사용
     1. CLASSFICATION - (분류모델)
     2. REGRESSION - (회귀모델)
2. `UNSUPERVISED LEARNING` (비지도학습)
   - 데이터에 정답이 없는경우 사용
     1. DIMENSIONALLY REDUCTION - (차원축소)
     2. CLUSTERING - (군집분석)
3. `REINFORCEMENT LEARNING`

---

### 지도학습과 비지도학습의 차이

- 지도학습은 정답을 부여 후 정답에 따라 분류
- 비지도학습은 분석 후 스스로 분류

---

## SUPERVISED learning

- Input data에 대한 정답을 예측하기 위해 학습
  - (`Function approximator`)
- 데이터에 정답(Label, Target)존재
- Output 형태에 따라 회귀분석, 분류 분석으로 나눔
  - 회귀분석 (Regression) : Output이 실수 영역 전체에서 나타남
  - 분류분석 (Classfication) : Output이 class에 해당하는 불연속값으로 나타남

---

## Unsupervised learning

- Input data 속에 숨어있는 규칙성을 찾기 위해 학습
  - `(shorter Description)`
- 데이터에 정답(Label, Target) 존재하지 않음
  - 군집분석 (Clustering Algorithm)
  - 차원축소 (Dimensionality reducton or Compression)

```
Column = Attribute = Dimension = Feature
```

---

## Reinforcement

- Trial & Error 등을 통해 학습
  - `(Sequential decision making)`

---

## 학습?

- 실제정답과 예측결과 사이의 오차를 줄여나가는 최적화 과정

Model's Capacity => 함수의 차수에 따라 올라감

- Capacity의 극대화 => Overfitting 발생 => Generalization error 증가 => 새로운 데이터에 잘 대응하지 못함

### 새로운 데이터들에 대해서도 좋은 결과를 내게하려면?

1. Cross Validation (교차검증)

   - 데이터를 3개의 그룹으로 나눈다

   1. 60%의 Traning data로 모델 학습시킨다
   2. 20%의 Validation data로 모델(or Hype parameter)을 최적화/선택(Tune)한다
   3. 20%의 Test data로 모델을 평가(Test only, no more tune)한다

2. (Stratified) K-Fold cross validation - (후보 모델 간 비교 및 선택을 위한 알고리즘)
   - K => 숫자를 직접 정함
   - CV
     - `cross validation`
     - compution vision
     - curriculum vision

---

<!-- 23.03.13 -->

## 선형 회귀

- **종속 변수 y**와 **한 개 이상의 독립 변수(설명 변수)x**사이의 선형 상관 관계를 모델링하는 회귀분석 기법
  - `정답이 있는 데이터의 추세`를 잘 설명하는 `선형 함수`를 찾아` x에 대한  y`를 예측
  - 1개의 독립변수가 1개의 종속변수에 영향 -> 단순 회귀분석
  - 2개 이상의 독립변수가 1개의 종속변수에 영향 -> 다중 회귀분석

== 가장 적합한 세타를 찾는게 목표

### cost Function (비용함수)

- 예측값의 실제 값의 `차이`를 기반으로 모델의 성능을 예측하기 위한 함수

1. 평균제곱오차함수(MSE function)

   - 회귀분석에서 활용하는 cost function중 가장 대표적
   - 오차를 제곱한 후 평균값을 구해 0에 수렴 할 수록 신뢰도 올라감

     - J세타 = 비용함수

2. Gradient Descent Algorithm(경사 하강법)
   - **Gradient** => 모든 변수(세타)의 편미분을 벡터로 정리한 것(=함수의 기울기, 경사)
   - 편미분 => 변수가 2개이상이 함수를 미분할 때 미분 대상의 변수 외에 나머지 변수를 상수처럼 고정시켜 미분

- Learning Rate (학습률) == 보폭(stepsize)
  - 얼마나 큰 보폭으로 움직일지를 결정해주는 값

---

    1. 변수 세타의 초기값을 설정
    2. 현재 변수값에  대응되는 Cost function 경사도 계산(미분)
    3. 변수를 경사 방향(Gradient의 음의방향)으로 움직여 다음 변수 값으로 설정
    4. 1~3을 반복하며 Cost function이 최소가 되도록 하는 변수값으로 근접해 나간다
       - 전체 Cost 값이 변하지 않거나 매우 느리게 변할 때까지 접근

HPO -> 멀티값을 가장 적합한 값으로 찾아내는것

==> (`H`yper `P`arameter `O`ptimization)

---

# Logistic Regression (로지스틱 회귀)

### `성능지표로 Mean Squared Error가 아닌, 분류를 위한 Cost function인 Cross-entropy를 활용`

- `이진 분류 문제`를 해결하귀 위한 모델
  - `Sigmoid function` 을 이용하여 기본적으로 특정 Input data 가 양성 class 에 속할 확률을 계산
  - `Sigmoid function` 의 정확한 y값을 받아 양성일 확률을 예측하거나, 특정 cutoff를 넘는 경우 양성, 그 외에는 음성으로 예측가능

---

# Softmax Algorithm (소프트 맥스 알고리즘)

- 다중 클래스 분류 문제를 위한 알고리즘 (일종의 함수)
  - model 의 output에 해당하는 logit(score)을 각 클래스에 소속될 확률에 해당하는 값들의 벡터로 변환해준다.
  - Logistic regression를 변형/발전시킨 방법으로, binary class 문제를 벗어나 multiple class 문제로 일반화시켜 적용할 수 있다.

---

<!-- 23.03.14 -->

TPR(True Postive Rate)

- 진양성율(실제 참인 것 중 모델이 참이라고 예측한 비율)

```
TP/TP+FN
```

FPR(False Postive Rate)

- 위양성율(실제 거짓인 것 중 모델이 참이라고 예측한 비율)

```
FP/TN+FP
```

# ROC Curve

Receiver operating characteristic Curve

- 아래쪽 면적
  - AUC (Area Under the ROC Curve)
  - 면적이 넓을수록 신뢰도 상승

---

# SVM_1 (Supprot Vector Machine)

- Margin을 최대화하는 결정 경계(면)을 찾는 기법
- 패턴 인식을 위한 지도 학습 모델 분류 뿐만 아니라 회귀에도 사용 가능
- 딥러닝 이전에 뛰어난 성능으로 많은 주목을 받은 머신러닝 알고리즘

---

# SVM_2 (Soft-Margin SVM)

- 현실에서는 매우 `엄격하게 2개의 클래스로만 데이터를 분리하기 어려움`
  - 소수의 noise로 인해 결정경계를 찾지 못할 수도 있음

--> 서포트 벡터가 위치한 `Plus & Minus plane에 약간의 여유번슈(Slack Variable)을 두는 것`으로 해결

---

# SVM_3 (Kernel Support Vector Machines)

- 데이터가 선형적으로 분리되지 않을 경우
- 커널함수를 사용

--> Original data가 놓여있는 차원을 `비션헝 매핑을 통해 고차원 공간으로 변환`하는 것으로 해결

---

## svm 모듈에 들어있는 하이퍼 파라미터

1. C
   - C값이 커지면 커질수록 오버피팅 날 확률 올라감
2. kernel
   - RBF(Radial Basis Function) 방사기저함수 =>가우시안커널
3. gamma
   - gamma가 커질수록 반지름은 줄어들고 작아질수록 커짐
     - gamma가 커질수록 오버피팅 날 확률 올라감

---

# Feature Normalization (Scaling)

1. Min-Max normalization

```
new(X) = old(x) - min(열)/max(열) - min(열)
```

- min(열) = 0
- max(열) = 0

---

2. Standardization (표준화)

```
new(X) = old(x) - mean(열)/std(열)
```

- mean(열) = 0
- std(열) = 1

---

# K-Nearest Neighbor Algorithm (KNN, K-최근접이웃 알고리즘)

- 기존의 가까운 이웃 데이터를 살펴 새로운 데이터를 분류하는 알고리즘

---

<!-- 23.03.15 -->

# Tree-based Ensemble models

## ensemble

- 주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후
- 이러한 예측 모형의 예측 결과를 종합하여 하나의 최종 예측결과를 도출해 내는 방법
- 다양한 모델이 문제 공간의 다은 측면을 보면서 각기 다른 방식으로 잘못된 부분이 있다고 가정(모델별로 약점을 보완)

---

## Decision Tree (의사결정나무)

- 메인 가지에서 가지치기를 하며 점점 정보를 좁혀나감
- 이해하기 쉽고 해석도 용이하나 입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라질 수 있음 & `과적합이 쉽게 발생`

---

## XG Boost (Extreme Gradient Boosting)

- Boosting
  - weak learner들을 strong learner로 변환시키는 알고리즘
- 대용량 분산처리를 위한 `그래디언트 부스팅` 오픈소스 라이브러리
- `의사결정나무`에 `Boosting기법`을 적용한 라이브러리
  - 빠르고 유연한 지도 학습 알고리즘, Kaggle등에서 우승한 많은 팀들이 선택한 알고리즘

## Boosting Algorithm

1. AdaBoost (Adaptive Boosting)
   - 데이터를 바탕으로 여러 weak learner들을 반복적으로 생성
   - 매번 앞선 learner 가 잘못 예측한 데이터에 가중치를 부여하고 학습
   - 최종적으로 만들어진 strong learner 를 이용해 실제 예측 진행

- AdaBoost는 높은 weight를 가진 data point가 존재하게 되면 성능이 크게 떨어지는 단점이 있음

---

2. Gradient Boosting

- 경사 하강법을 사용하여 AdaBoost보다 성능을 개선
- 학습 성능은 좋으나 모델의 학습시간이 오래 걸리는 단점 발생

3. XG Boost (Extreme Gradient Boosting)

- 병렬 처리 기법을 적용하여 Gradient Boost보다 학습속도를 크게 올림

---

# Unsupervised (비지도학습)

## K-Means Algorithm

1. K 개의 임의의 중심값을 고른다
2. 각 데이터마다 중심값까지의 거리를 계산하여 가까운 중심값의 클러스터에 할당한다
3. 각 클러스터에 속한 데이터들의 평균값으로 각 중심값을 이동시킨다
4. 데이터에 대한 클러스터 할당이 변하지 않을 때 까지 2와 3을 반복한다

## Dimension Reduction Algorithm

1. PCA (Principal Component Analysis, 주성분 분석)
   - 차원축소를통해 최소차원의 정보로 원래 차원의 정보를 모사하는 알고리즘
     - 차원축소 : 고차원 벡터에서 일부 차원의 값을 모두 0으로 만들어 저차원 벡터로 줄이는 것
       - 이때 원래의 고차원 벡터의 특성을 최대한 살리기 위해 가장 분산이 높은 방향으로 회전 변환을 진행
