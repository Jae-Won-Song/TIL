# Machine Learning

- 명시적인 프로그래밍 없이 `컴퓨터가 학습하는 능력`을 갖추게 하는 일
- 데이터로부터 `경험을 축적`하여 `스스로 학습`할 수 있는 모델개발이 목적
- 전통적인 통계모델 : 개별적 데이터에 대한 하나의 모델을 개발하는것이 목적
- 자료가 더 많이 축적될수록 예측력은 강해지고 알고리즘은 데이터의 업데이트에 적응하여 `자동화` 가능

---

# 머신러닝의 종류
- 지도학습 (Supervised learning)
  - 일반화 선형모델
    - 선형회귀의 발전된 형태
    - 다양한 분산과 함수를 지원해 효과적으로 데이터 모델링
  - 의사결정 트리
    - 집단을 변수에 대해 동질적인 하위 집단 간에 점진적으로 분할해 학습하는 방법
  - 랜덤 포레스트
    - 다수의 의사결정 트리를 학습한 다음 트리 전반에 걸친 평균을 구해 예측을 산출
  - 평균 프로세스는 데이터의 불규칙 잡음을 걸러내는데 효과적
  - 신경망 딥러닝
    - 데이터의 고수준 패턴을 복합적인 다층 네트워크로 모델링하는 방법
    - 현재까지 머신러닝의 가장 어려운 문제를 해결할 수 있는 방법론


- 비지도학습
    - 클러스터링
      - 각 개체를 기준 변수에 따라 계산된 유사성을 기준으로 그룹화하는 기법
      - 가장 널리 사용되는것이 K-평균
    - 차원축소
      - 대상 변수의 수를 줄이는 프로세스
      - 데이터의 축약을 통해 효율적인 특징추출을 시도하는 방법으로 PCA, tsne등이 대표적
    - 비정상탐지
      - 예상치 못한 이벤트 또는 결과를 식별하는 방법으로 DBSCAN이 대표적
      - 프로세스, 보안, 사기등의 분야에서 일반적이지 않은 거래를 찾아냄


---
`머신러닝 프로세스 과정`
1. 데이터로딩
2. 데이터 속성, 형태 등등
   1. 문자가 존재 => 숫자(정수 인코딩 // 원-핫인코딩)로 변환
   2. 결측치가 존재 => imputation => 최소/최대, 최빈, 평균, 중앙/ 해당컬럼 삭제
3. 데이터 분할
   1. train_test_split => train vs test 비율을 분할하는 방법
   2. Validition (검증데이터셋)
      1. Cross-validation
         1. K-fold
4. 알고리즘을 선택 
   1. 분류 => classfier 
      1. y변수가 범주형(명목형)
   2. 회귀 => Regressor
      1. y변수가 연속형(비율척도)
5. 평가 
   1. 분류 => 혼동행렬
   2. 회귀 => MSE, RMSE, MAE, MAPE MSLE
6. Hyper-Parameter Tuning
   1. Manual search
   2. Grid search
   3. Random search

---


`머신러닝에서 데이터를 분할하는 이유`
- 과대적합을 피하고 편향을 제거한 데이터로 모델 성능을 평가하기 위해

---

`딥러닝 프로세스 과정`
1. Text encoding
2. Image encoding
3. 딥러닝 모델링
4. 시계열

---
## Hold_Out


  ### 과대적합(overfitting)
- 가진 정보를 모두 사용해서 **너무 복잡한 모델**을 만드는 것
- 모델이 훈련 세트의 각 샘플에 너무 가깝게 맞춰져서 새로운 데이터에 일반화되기 어려울 때 나타남


  ### 과소적합(underfitting)
- 머신러닝 모델이 충분히 복잡하지 않아(최적화가 제대로 수행되지 않아) 학습 데이터의 구조/패턴을 정확히 반영하지 못하는 문제